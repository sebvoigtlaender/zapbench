{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "G1Pc3oeULxKL"
      },
      "source": [
        "# Train and evaluate"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mVQJ85TyZa7x"
      },
      "source": [
        "This tutorial explains how to train and eval forecasting methods on ZAPBench in a framework agnostic way. For this, we will be using [`grain`, a library for reading and processing ML training data](https://github.com/google/grain).\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VFVIA-6tjKEJ"
      },
      "source": [
        "## Training"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uar0yiktkBum"
      },
      "source": [
        "`zapbench` provides data sources that are compatible with `grain`, e.g.:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mhQAMdomm9JK"
      },
      "outputs": [],
      "source": [
        "!pip install git+https://github.com/google-research/zapbench.git#egg=zapbench"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QhH3KuMtk_MT"
      },
      "outputs": [],
      "source": [
        "from zapbench import constants\n",
        "from zapbench import data_utils\n",
        "from zapbench.ts_forecasting import data_source\n",
        "\n",
        "\n",
        "condition_name = 'turning'  # can be any name in constants.CONDITION_NAMES\n",
        "num_timesteps_context = 4  # 4 for short context, 256 for long context\n",
        "split = 'train'  # change to 'val' for validation set, e.g., for early stopping\n",
        "\n",
        "config = data_source.TensorStoreTimeSeriesConfig(\n",
        "    input_spec=data_utils.adjust_spec_for_condition_and_split(\n",
        "        condition=constants.CONDITION_NAMES.index(condition_name),\n",
        "        split=split,\n",
        "        spec=data_utils.get_spec('240930_traces'),\n",
        "        num_timesteps_context=num_timesteps_context),\n",
        "    timesteps_input=num_timesteps_context,\n",
        "    timesteps_output=constants.PREDICTION_WINDOW_LENGTH,\n",
        ")\n",
        "source = data_source.TensorStoreTimeSeries(config)\n",
        "\n",
        "print(f'{len(source)=}')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WNzr8t6_od6Z"
      },
      "source": [
        "We'll briefly setup pretty-printing, and then index into the data source to get elements:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DR2mtKo1pDnm"
      },
      "outputs": [],
      "source": [
        "!pip install treescope"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mznamK0To7-n"
      },
      "outputs": [],
      "source": [
        "import treescope\n",
        "treescope.basic_interactive_setup(autovisualize_arrays=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_CxkG54Xozzh"
      },
      "outputs": [],
      "source": [
        "source[0]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Y86dC5PupINB"
      },
      "source": [
        "... when indexing into the data source, we get `series_input`, i.e., past activity of `num_timesteps_context` length, and `series_output`, 32 timesteps of subsequent activity (the prediction horizon used in ZAPBench)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vEO_xsi0p_1N"
      },
      "source": [
        "By enabling `prefetch` on `data_source.TensorStoreTimeSeries`, we can load the entire data into memory upfront. This makes indexing significantly faster once the source has been initialized."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yJ4vw4oMqrm6"
      },
      "outputs": [],
      "source": [
        "import random\n",
        "\n",
        "source = data_source.TensorStoreTimeSeries(config, prefetch=False)  # Default"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PKYe1kJvkAQi"
      },
      "outputs": [],
      "source": [
        "%%timeit\n",
        "_ = source[random.randint(0, len(source)-1)]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lktbCrGZqwQ7"
      },
      "outputs": [],
      "source": [
        "source = data_source.TensorStoreTimeSeries(config, prefetch=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_zlzE3uqqxp_"
      },
      "outputs": [],
      "source": [
        "%%timeit\n",
        "_ = source[random.randint(0, len(source)-1)]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wAT0jhfe_f_p"
      },
      "source": [
        "We can also create a data source that combines data from all training conditions (should take about a minute to prefetch):"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "S379VzI6_d6y"
      },
      "outputs": [],
      "source": [
        "sources = []\n",
        "\n",
        "# Iterate over all training conditions (excludes 'taxis'), and create\n",
        "# data sources.\n",
        "for condition_id in constants.CONDITIONS_TRAIN:\n",
        "  config = data_source.TensorStoreTimeSeriesConfig(\n",
        "      input_spec=data_utils.adjust_spec_for_condition_and_split(\n",
        "          condition=condition_id,\n",
        "          split='train',\n",
        "          spec=data_utils.get_spec('240930_traces'),\n",
        "          num_timesteps_context=num_timesteps_context),\n",
        "      timesteps_input=num_timesteps_context,\n",
        "      timesteps_output=constants.PREDICTION_WINDOW_LENGTH,\n",
        "  )\n",
        "  sources.append(data_source.TensorStoreTimeSeries(config, prefetch=True))\n",
        "\n",
        "# Concatenate into a single source.\n",
        "source = data_source.ConcatenatedTensorStoreTimeSeries(*sources)\n",
        "\n",
        "f'{len(source)=}'"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "m14JiTXJ1DRG"
      },
      "source": [
        "Next, we set up an index sampler and construct a data loader with `grain`:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "easEO6aY1Cu_"
      },
      "outputs": [],
      "source": [
        "import grain.python as grain\n",
        "\n",
        "\n",
        "batch_size = 8\n",
        "num_epochs = 1\n",
        "shuffle = True\n",
        "\n",
        "index_sampler = grain.IndexSampler(\n",
        "    num_records=len(source),\n",
        "    num_epochs=num_epochs,\n",
        "    shard_options=grain.ShardOptions(\n",
        "        shard_index=0, shard_count=1, drop_remainder=True),\n",
        "    shuffle=shuffle,\n",
        "    seed=101\n",
        ")\n",
        "\n",
        "data_loader = grain.DataLoader(\n",
        "    data_source=source,\n",
        "    sampler=index_sampler,\n",
        "    operations=[\n",
        "        grain.Batch(\n",
        "            batch_size=batch_size, drop_remainder=True)\n",
        "    ],\n",
        "    worker_count=0\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SWrrKBcC2Tm1"
      },
      "source": [
        "We can iterate over the data loader which will get elements with a batch dimension in random order for `num_epochs`:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nxH1JlSE2S5u"
      },
      "outputs": [],
      "source": [
        "from tqdm import tqdm\n",
        "\n",
        "\n",
        "for element in tqdm(data_loader):\n",
        "  #\n",
        "  # ... train model with element\n",
        "  #\n",
        "  continue\n",
        "\n",
        "element"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VfDi9Inz2tNx"
      },
      "source": [
        "`grain` has many useful features -- for example, we can easily add operations to the data loader to adjust shapes, or add augmentations. More details are in [grain's DataLoader guide](https://google-grain.readthedocs.io/en/latest/tutorials/data_loader_tutorial.html)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5dR8YHzt3yo4"
      },
      "source": [
        "## Evaluation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xz_l24rcji2C"
      },
      "source": [
        "Say we have trained a new baseline, how do we evaluate it?\n",
        "\n",
        "We are going to use the mean baseline from the manuscript as an example: It can easily be re-implemented in NumPy and does not require any training."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Iv5mQQQYiNtB"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "\n",
        "\n",
        "def f_mean(past_activity: np.ndarray) -\u003e np.ndarray:\n",
        "  \"\"\"Mean baseline\n",
        "\n",
        "  Args:\n",
        "    past_activity: Past activity as time x neurons matrix.\n",
        "\n",
        "  Returns:\n",
        "    Predicted activity calculated by taking the per-neuron mean across time and\n",
        "    repeating it for all 32 timesteps in the prediction horizon.\n",
        "  \"\"\"\n",
        "  return past_activity.mean(axis=0).reshape((1, -1)).repeat(\n",
        "      constants.PREDICTION_WINDOW_LENGTH, axis=0)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "m5gozhtfB1YU"
      },
      "source": [
        "For inference, we create a data source containing the full trace matrix, and index it as described in [the manuscript](https://openreview.net/pdf?id=oCHsDpyawq) (section 3.2) to compute metrics."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Qlywvmcqi0RZ"
      },
      "outputs": [],
      "source": [
        "infer_source = data_source.TensorStoreTimeSeries(\n",
        "    data_source.TensorStoreTimeSeriesConfig(\n",
        "        input_spec=data_utils.get_spec('240930_traces'),\n",
        "        timesteps_input=num_timesteps_context,\n",
        "        timesteps_output=constants.PREDICTION_WINDOW_LENGTH,\n",
        "    ),\n",
        "    prefetch=True\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NVMOhD1KD8t8"
      },
      "outputs": [],
      "source": [
        "from collections import defaultdict\n",
        "\n",
        "from connectomics.jax import metrics\n",
        "\n",
        "\n",
        "# Placeholder for results\n",
        "MAEs = defaultdict(list)\n",
        "\n",
        "# Iterate over all conditions, and make predictions for all contiguous snippets\n",
        "# of length 32 in the respective test set.\n",
        "for condition_id, condition_name in tqdm(enumerate(constants.CONDITION_NAMES)):\n",
        "  split = ('test' if condition_id not in constants.CONDITIONS_HOLDOUT\n",
        "           else 'test_holdout')\n",
        "  test_min, test_max = data_utils.adjust_condition_bounds_for_split(\n",
        "      split,\n",
        "      *data_utils.get_condition_bounds(condition_id),\n",
        "      num_timesteps_context=num_timesteps_context)\n",
        "\n",
        "  for window in range(\n",
        "      data_utils.get_num_windows(test_min, test_max, num_timesteps_context)):\n",
        "    element = infer_source[test_min + window]\n",
        "\n",
        "    predictions = f_mean(element['series_input'])\n",
        "    mae = metrics.mae(predictions=predictions, targets=element['series_output'])\n",
        "\n",
        "    MAEs[condition_name].append(np.array(mae))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MG1BcCHseex2"
      },
      "source": [
        "... let's plot our results:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3RlJY6t5HaKB"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "\n",
        "steps_ahead = np.arange(32) + 1\n",
        "\n",
        "for condition_name in constants.CONDITION_NAMES:\n",
        "  mae = np.stack(MAEs[condition_name]).mean(axis=0)  # Average over windows\n",
        "  plt.plot(steps_ahead, mae, label=condition_name)\n",
        "\n",
        "plt.title('mean baseline, short context')\n",
        "plt.xlabel('steps predicted ahead')\n",
        "plt.ylabel('MAE')\n",
        "plt.ylim((0.015, 0.06))\n",
        "plt.xlim(1, 32)\n",
        "plt.legend(bbox_to_anchor=(1.05, 1), loc='upper left')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3V6o7v5VXTb2"
      },
      "source": [
        "Finally, we briefly check that these results match the ones in the manuscript:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rkH6JVuuXTBF"
      },
      "outputs": [],
      "source": [
        "from connectomics.common import ts_utils\n",
        "import pandas as pd\n",
        "\n",
        "\n",
        "# Load dataframe with results reported in the manuscript.\n",
        "df = pd.DataFrame(\n",
        "    ts_utils.load_json(f'gs://zapbench-release/dataframes/20250131/combined.json'))\n",
        "df.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "X1BA4QHkXv0F"
      },
      "outputs": [],
      "source": [
        "for condition_name in constants.CONDITION_NAMES:\n",
        "  mae = np.stack(MAEs[condition_name]).mean(axis=0)\n",
        "  mae_df = df.query(\n",
        "      f'method == \"mean\" and context == 4 and condition == \"{condition_name}\"'\n",
        "  ).sort_values('steps_ahead')['MAE'].to_numpy()\n",
        "  np.testing.assert_array_almost_equal(mae, mae_df, decimal=8)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "private_outputs": true,
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
